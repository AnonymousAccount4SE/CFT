# License
<<< EOF
#
# CFT - an interactive programmable shell for automation 
# Copyright (C) 2020-2022 Roar Foshaug
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, version 3 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>
#
>>> EOF
/License

# Readme
<<<< EOF
-------------------------------------------------------------
Simple script library for interfacing a running
ElasticSearch node (or cluster via one of its nodes),
through the REST API.

Initially asks for host+port on format xxx:NNN, then stores
this in database. 

To change to different host name, call MyHost(true)

#Note: the Util:CURL function has problems with JSON data
# on windows. Works fine on Linux / WSL

-------------------------------------------------------------
>>>> EOF
/Readme

# Get/set target hostname
# --
    P(1,false) => change

    Db2:Get("ElasticAPI","host") => name

    if (name==null || change) {
        println("Enter elasticsearch host[:port] (port defaults to 9200)")
        
        readLine("host:port") => name
        if(!name.contains(":")) name=name+":9200"
        Db2:Set("ElasticAPI","host",name)
    }
    name
/MyHost



# Change target hostname
# --
    MyHost(true)
/ChangeMyHost




# Create Kibana URL
# --
    MyHost.before(":") + ":5601/app/kibana"
/KibanaURL



# List indexes, either just names or specific index by entering name (glob)
# --
	P(1,Input("Enter name as glob expression").setCurr("*").get)=>globExpr
	showNamesOnly=(globExpr=="*")
	regex=Glob(globExpr,true).regex

    Util:CURL("http",MyHost,"GET","/_cat/indices?format=json&pretty=true",null,false).stdout => json
	data=JSON:Parse(json)

	# Sort list of indexes by name
	data = Inner {
		data->ix out(Str(ix.index,ix)) | _.sort->x out(x.data)
	}

	# generate output
	data->ix
		name=ix.index
		assert(regex.match(name))
		if (showNamesOnly) println(name) else JSON:PP(ix,true)
/ListIndexes


# Create index with lifecycle management
#   NOTE: does not really work, must use Index Templates to control lifecycle
#   for all indexes created via rollover.
# --
    P(1,Input("Index name (-000001 will be added)").get) => indexName

	error(Regex("^.*-\d.*$").match(indexName),
		"Invalid index name, should not contain -N where N is a digit")
	
	P(2,Input("index.lifecycle.name").get) => ilpName
	P(3,Input("index.lifecycle.rollover_alias").setCurr(indexName).get) => ilpAlias

	indexName=indexName+"-000001"


    Dict.set("settings",Dict
		.set("number_of_shards",1)
		.set("number_of_replicas",1)
		.set("index.lifecycle.name", ilpName)
		.set("index.lifecycle.rollover_alias", ilpAlias)
	).set("aliases",Dict
		.set(ilpAlias, Dict
			.set("is_write_index", true)
		)
	)
	=> data

    url = "/"+indexName+"?pretty" 

	println("PUT " + url)
	JSON:PP(data,true)
	Lib:HardConfirm("Continue")

    Util:CURL("http",MyHost,"PUT", url, data, true)
/CreateIndex



# Create index managed via index template (see CreateIndexTemplate)
# https://www.elastic.co/guide/en/elasticsearch/reference/7.7/getting-started-index-lifecycle-management.html#ilm-gs-create-policy
# --
    P(1,Input("Index name (-000001 will be added)").get) => indexName
	P(2,Input("Index alias").get) => alias

	error(Regex("^.*-\d.*$").match(indexName),
		"Invalid index name, should not contain -N where N is a digit")
	
	indexName=indexName+"-000001"

	Dict
		.set("aliases",Dict
			.set(alias, Dict
				.set("is_write_index", true)
			)
		)
	=>data

    url = "/"+indexName 

	println("PUT " + url)
	JSON:PP(data,true)
	Lib:HardConfirm("Continue")

    Util:CURL("http",MyHost,"PUT", url, data, true)
/CreateIndexForTemplate



# Add alias to index
# --
	P(1,Input("Index name").get) => indexName
	P(2,Input("Index alias").setCurr(indexName).get) => alias
	
	Dict.set("actions",List(
		Dict.set("add", Dict.set("index",indexName).set("alias",alias))
	))
	=> data

	url="/_aliases"
	println("POST " + url)
	JSON:PP(data,true)
	Lib:HardConfirm
	
	Util:CURL("http",MyHost,"POST", url, data, true)
/AddIndexAlias



# Locate newest index that matches pattern, by looking for -NNNNNN version number
# at the end of the name, selecting the highest number. 
#
# Checks at most every minute, caches result in Db2.
# --
	P(1,Input("Enter index pattern").get)=>pattern

	dbId="newest_index:"+pattern
	timerId=Sys.scriptId+"_locateNewestIndex"

	value=Db2:Get(Sys.scriptId,dbId)

	if (value==null || Util:HasTimedOut(timerId,60)) {	
		regex=Glob(pattern,true).regex
		Util:CURL("http",MyHost,"GET","/_cat/indices?format=json&pretty=true",null,false).stdout => json
		data=JSON:Parse(json)

		# Iterate over all index objects, find one with highest version		
		maxValue=-1
		maxValueIndex=null
		Inner {
			data->ix 
				name=ix.index
				assert(regex.match(name))
				if (Regex("^.*-\d+$").match(name)) {
					num=name.afterLast("-").parseInt
					if (num > maxValue) {
						maxValue=num
						maxValueIndex=name
					}
				}
		}
		error(maxValueIndex==null,"No numbered indexes for pattern " + pattern)

		# Success
		value=maxValueIndex

		# Store in database
		Db2:Set(Sys.scriptId, dbId, value)

		# Set timer
		Util:SetTimeMark(timerId)
	}

	value	
/LocateNewestIndex


# Add data WITH timestamp in order to enable time searches
# If the given index name is a pattern, we regularly search for the newest
# index that matches that pattern, by looking for -NNNNNN at the end of the
# name, selecting the highest number. 
# --
	P(1,Input("Index pattern or name").get) => indexPattern
	P(2,Input("Sleep ms").get.parseInt) => delay


	i=0
	loop
		i=i+1
		Util:Counter("customer_id") => recordId

		value=(Lib.Math.sin(i)*100).i

		Date.sub(Date.Duration.hours(1)).setFormat("yyyy-MM-dd'T'HH:mm:ss").fmt => timestamp
	

		data = Dict.set("name","Roar")
			.set("timestamp", timestamp)
			.set("value",value)

		indexName=if(indexPattern.contains("*")) LocateNewestIndex(indexPattern) else indexPattern

		println("i="+i + " " + indexName)
	
		url="/" + indexName + "/_doc/"+recordId
		println("POST " + url)
		JSON:PP(data,true)


		Util:CURL("http",
			MyHost,
			"POST",
			url, 
			data,
			false
			)

		Sys.sleep(delay)
/AddIndexData



# Add data into time-separated indexes, as decided via Date format string
# --
	P(1,Input("Index name").get) => indexName
	P(2,Input("Sleep ms").get.parseInt) => delay
	P(3,Input("Date/Time pattern").setCurr("yyyy-MM-dd-HH").get)=>dateFormat

	i=0
	loop
		i=i+1
		Util:Counter("customer_id") => recordId

		value=(Lib.Math.sin(i)*100).i

		Date.sub(Date.Duration.hours(1)).setFormat("yyyy-MM-dd'T'HH:mm:ss").fmt => timestamp
		data = Dict.set("name","Roar")
			.set("value",value)
			.set("timestamp",timestamp)

		println("i="+i+" "+indexName)

		indexDatePart=Date.setFormat(dateFormat).fmt

		url="/" + indexName + "-" + indexDatePart + "/_doc/"+recordId
		println("POST " + url)
		JSON:PP(data,true)

		Util:CURL("http",
			MyHost,
			"POST",
			url, 
			data,
			false
			)

		Sys.sleep(delay)
/AddMultiIndexData





# Create or modify Index Lifecycle Policy - using hours for testing
# https://www.elastic.co/guide/en/elasticsearch/reference/7.7/getting-started-index-lifecycle-management.html#ilm-gs-create-policy
# --
    P(1,Input("Index Lifecycle Policy name").get) =>ilpName
    P(2,Input("Rollover hours").get.parseInt)=>rolloverHours
    P(3,Input("Expiration hours").get.parseInt)=>expirationHours
	
	# Seems the rollover-part is not changed when running function
	# repeatedly, while the delete-part is.
	#
	# Deleting the policy from inside Kibana, then recreating it
	# with this function, we get it correct!

<<<<<<<< EOF
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_age": "<< rolloverHours >>h",
            "max_size": "2mb"
          }
        }
      },
      "delete": {
        "min_age": "<< expirationHours >>h",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
>>>>>>>> EOF
	=> lines
	lines.mergeExpr->line out(line.trim) | _.concat(" ") => json
    url="/_ilm/policy/" + ilpName
   
	# Show JSON
	println("PUT " + url)
	JSON:PP(JSON:Parse(json),true)
	Lib:HardConfirm

    Util:CURL("http",MyHost,"PUT",url,json,false).stdout => output
	JSON:PP(JSON:Parse(output),true)
/ILPCreate



# Check lifecycle progress
# --
	P(1,Input("Index name or pattern").setCurr("*").get) => index
    json = Util:CURL("http",MyHost,"GET","/" + index + "/_ilm/explain").stdout
	JSON:PP(JSON:Parse(json))
/ILPProgress




# List ILP's
# --
	Util:CURL("http",MyHost,"GET","/_ilm/policy").stdout => json
	JSON:PP(JSON:Parse(json))
/ILPList


# Create Index Template
#
# https://www.elastic.co/guide/en/elasticsearch/reference/7.7/indices-templates.html
# https://www.elastic.co/guide/en/elasticsearch/reference/7.7/getting-started-index-lifecycle-management.html#ilm-gs-create-policy
#
# --
    P(1,Input("Index pattern").get) => indexPattern
	P(2,Input("Index template name").get) => templateName
	P(3,Input("index.lifecycle.name").get) => lifecycleName
	P(4,Input("index.lifecycle.rollover_alias").get) => alias

	Dict.set("index_patterns",List(
			indexPattern
			)
		)
		.set("settings",Dict
			.set("number_of_shards", 1)
			.set("number_of_replicas", 1)
			.set("index.lifecycle.name", lifecycleName)
			.set("index.lifecycle.rollover_alias", alias)
		)
	=>data
	url="/_template/" + templateName + "?pretty"

	println("POST " + url)
	JSON:PP(data,true)
	Lib:HardConfirm("Continue")

    Util:CURL("http",MyHost,"POST",url,data,true)
        
/CreateIndexTemplate


# Delete time-stamped index, named on format <prefix><date>
# --
    P(1,"xxx-")=>prefix
    P(2,"yyyy-MM-dd")=>dateFormat
    P(3,Date.sub(Date.Duration.days(30)))=>cutoffDate
	P(4,false)=>enable
		# If false, we display what indexes would be deleted
		# If true, they are actually deleted
	
	json=Util:CURL("http",MyHost,"GET","/_cat/indices?format=json&pretty=true",null,false).stdout
	data=JSON:Parse(json)

	# should be a list of index objects

	error(getType(data) != "List", "Expected list, got " + getType(data))

	# Iterate over indexes, and from their name, parse the date/time.
	# For those older than cutoffDate: delete
	data->ix
		indexName=ix.index
		assert(indexName.startsWith(prefix))
		indexName.sub(prefix.length) => datePart
		println("Index-name: " + indexName + " date/time-part: " + datePart)
		Date.setFormat(dateFormat).parse(datePart) => indexDate

		if (indexDate.before(cutoffDate)) {
			println("* Deleting index " + indexName)
			if (enable) Util:CURL("http",MyHost,"DELETE","/"+indexName,null,false)
		} 
/DeleteDatedIndexes


# Delete winlogbeat-6.4.2-yyyy.MM.dd, metricbeat-7.2.0-yyyy.MM.dd and filebeat-7.9.3-yyyy.MM.dd older than 90 days
# --
	expiration=Date.sub(Date.Duration.days(30))
	ENABLE=false
	
	DeleteDatedIndexes("winlogbeat-6.4.2-", "yyyy.MM.dd", expiration, ENABLE)
	DeleteDatedIndexes("metricbeat-7.2.0-", "yyyy.MM.dd", expiration, ENABLE)
	DeleteDatedIndexes("filebeat-7.9.3-", "yyyy.MM.dd", expiration, ENABLE)
/Cleanup




# Test DeleteDatedIndexes
# --
	DeleteDatedIndexes("mix-", "yyyy-MM-dd-HH", Date.sub(Date.Duration.hours(4)), true)
/t




# Index Lifecycle Policy README
# --
<<<<<< EOF

Adding ILP directly to existing index
-------------------------------------
- If the index has no alias (watever that is), then add one with AddIndexAlias
- Verify in Kibana
- Create Index Lifecycle Policy
- Click on the index, and manage, add lifecycle policy

DOES NOT WORK (7.16)

The index had to be named xxx-N or we got an error
when it got time to do roll over. Called it main-1 

Note that the error above didn't show up before trying
to execute the ILP. Good thing it had short timeouts.

When it became time to roll over, it then created an 
index main-0000002, which was empty, and
after a further while, it reset the the main-1, purging
all content. 

So the ILP was doing something, just not anything
meaningful.
 
Setting up ILP when creating Index
----------------------------------
The CreateIndex function does this, setting up an alias and
coupling the index to an ILP. Seems to work.


Setting up ILP via Index Template
---------------------------------
NOTE: does not work! :-)

- Create index with some timestamp on format

		yyyy-MM-dd'T'HH:mm:ss

	** possibly not required for ILP, but good for displaying
	   data along time axis, to see if ILP does anything)

- When creating Kibana Index Pattern, use this field as time field
	This enables presenting data on time axis (when exploring etc)


- Create Index Template under Index management -> Index Templates
	- give index patterns for the indices it controls 


- Create Index Lifecycle Policy
	- Actions: Add policy to Index template


*** Use date pattern in indexes
-------------------------------
Requires external scripting to purge old data
BUT IT WORKS!
See functions

- AddMultiIndexData
- DeleteDatedIndexes

NOTE: it seems the Kibana Index pattern MUST be on the form "xxx-*"
and not just "xxx*". Bugix. 




>>>>>> EOF
	-> line println("| " + line)
/ILPReadme

